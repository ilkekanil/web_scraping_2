{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://easycep.com/kategori/cep-telefonu-1?sortingCriteria=2&page='\n",
    "\n",
    "# Titles corresponding to the information\n",
    "titles = [\n",
    "    'Yüz Tanıma', \n",
    "    'Parmak İzi Okuyucu', \n",
    "    'RAM', \n",
    "    'Hızlı Şarj', \n",
    "    'Ön Kamera Çözünürlüğü', \n",
    "    'Arka Kamera Çözünürlüğü', \n",
    "    'Ekran Boyutu', \n",
    "    'Hat Sayısı'\n",
    "]\n",
    "\n",
    "# List to store all the data for transering to excel\n",
    "all_data=[]\n",
    "\n",
    "# Iterate over pages from 1 to 11\n",
    "for page in range(1, 12):\n",
    "    url = base_url + str(page)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parsing HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the first part of the desired HTML\n",
    "        f_fragment = soup.find('div', class_='row g-3')\n",
    "        \n",
    "        # Control if the row is found\n",
    "        if f_fragment: \n",
    "            #Find the second fragment from the f_fragment\n",
    "            s_fragment = f_fragment.find_all('div', class_='product__typeOne')\n",
    "            \n",
    "            if s_fragment:\n",
    "                #Extract a and href to get the absolute url\n",
    "                for i in s_fragment:\n",
    "                    t_fragment = i.find('a')\n",
    "                    if t_fragment:\n",
    "                        href = t_fragment.get('href')\n",
    "                        # Updated URL\n",
    "                        absolute_url = urljoin(url, href)\n",
    "                        \n",
    "                        # New request to the desired site\n",
    "                        linked_page_request = requests.get(absolute_url)\n",
    "                        if linked_page_request.status_code == 200: #Check if the site is found\n",
    "                            linked_soup = BeautifulSoup(linked_page_request.content, 'html.parser')\n",
    "                            #To get the technical information table\n",
    "                            name = linked_soup.find('h2', class_='productDetail__tabList--title').text.strip()\n",
    "                            print(f'Isim: {name}')\n",
    "                            table = linked_soup.find_all('div', class_='productDetail__tabList--itemText')\n",
    "                            #Store the data \n",
    "                            data= {'Isim': name}\n",
    "                            \n",
    "                            # Check if we have the expected number of items\n",
    "                            if len(table) == len(titles):\n",
    "                                #Iterate titles over items of the table\n",
    "                                for each_title, item in zip(titles, table):  \n",
    "                                    print(f'{each_title}: {item.text.strip()}')\n",
    "                                    #Store the data into my list\n",
    "                                    data[each_title]= item.text.strip()\n",
    "                            else:\n",
    "                                #This part is to control the loop, if the numbers are not same\n",
    "                                print(f'Expected {len(titles)} items, but found {len(table)} items.')\n",
    "                                for each_title, item in zip(titles, table):\n",
    "                                    print(f'{each_title}: {item.text.strip()}')\n",
    "                                    #Store the data\n",
    "                                    data[each_title]= item.text.strip()\n",
    "                                for title in titles[len(table):]:\n",
    "                                    print(f'{title}: Data not found')\n",
    "                                    \n",
    "                            \n",
    "                            # Append the data to the all_data list\n",
    "                            #all_data.append(data)\n",
    "                            \n",
    "                        else:\n",
    "                            print('Third stage of HTML fragment not found')\n",
    "            else:\n",
    "                print('Second stage of HTML fragment not found')\n",
    "        else: \n",
    "            print('First stage of HTML fragment not found')\n",
    "    else:\n",
    "        print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "\n",
    "        \n",
    "# Convert the all_data list\n",
    "#df = pd.DataFrame(all_data, columns=['Isim'] + titles)\n",
    "\n",
    "# Save the Data to an Excel file\n",
    "#df.to_excel('output.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
